# Notes

## Command Tips
- Instead of going into the kubernetes documentation from the start, try using the resources inside kubectl
- By running `kubectl [command] --help | vim -`, you can most likely find the information before searching it up online
- When generating template kubernetes yaml files (i.e deployments), this can be done by using the --dry-run command
- For example: `kubectl create deployment test --dry-run=client -o yaml > deployment.yaml`
- If you would like to get the yaml for an existing resource - `k get pod example -n [namespace] -o yaml > pod.yaml`
- If you would like to update the default namespace being used for commands - `kubectl config set-context --current --namespace=[namespace]`

## General
- Kubernetes is considered the OS of the cloud.  Think of the nodes as a bunch of virtual machines who communicate with each other
- A control plane is what manages the scheduling, api calls, key value store (etcd), and management of nodes
- Pods are the smallest unit that exist inside within each node 
    - Pods are NOT just the container.  Each pod is an operating envrionment that has the ability to run one or more containers.
- Each node contains a kubelet that communicates with the control plane to provide health information on pod information
    - Additionally, they also contain a kube-proxy that allows them into maintain network rules on each node
- When working with containers in YAML files, there is a distinguishable difference between `command` and `args`
    - command acts as the entrypoint for the command to run (i.e ["/bin/sh", "-c"])
    - args supplies arguments to the command being run
    - The reason the "-c" in the example needs to be there is because it is not an argument, it is an option
    - If "-c" was specific inside the args field, it may be interpretted as a file instead

## Networking
- Each pod gets its own IP address
- By default pods can connect to all pods on all nodes
    - This can be limited through network policies
- Containers in pods can communicate with each other through localhost
- Each container must have their own unique port
- The CNI Plugin is a Container Networking Interface
    - Think of this plugin as a physical network card
    - It also handles the wiring the connections between each container
    - IP addresses are assigned and routes are set up using IPTables on nodes
    - Plugins: Cilium, Calico, Flannel
- To investigate the node vm's CNI, we used rdctl (rancher-desktop ctl): `rdctl shell bash`
- We then checked out /etc/cni/net.d/
- We saw the file 10-flannel.conflist and by using `cat` we observed that flannel was running on our cluster
- Services offer consistent addresses to access a set of pods
    - Since pods are ephemeral, you should not expect a pod to be long lasting
    - Pods are constantly changing and moved between nodes
    - The system needs a way to keep track of these constantly changing IP addresses
    - A service is a grouping of pods (i.e frontend nginx service)
    - The traffic goes through the frontend service, not directly to the pods
- To inspect the service - `kubectl get service`
- Types of services:
    - ClusterIP - Created by default when using `kubectl expose` command.  This IP won't change
    - NodePort - Exposes a port on each node allowing direct access to the service through any node's IP address (try to avoid) 
    - LoadBalancer - Creates a loadbalancer used to route traffic into the cluster
- If using the ClusterIP type, we would initially get the external-ip is none as kubernetes doesn't know how to get the IP
- Using Rancher Desktop, editing the service `kubectl edit service [service_name]` changing it to LoadBalancer get's an external-ip
- Previously, we were doing port forwards with the following: `kubectl port-forward pods/[pod_name] [target_port]`
- We can also do this port-forwarding to the exposed service instead: `kubectl port-forward services/[service_name] [target_port]` 
- The problem with this now is that if we close the port forwarding, this will break the outside connections
- To fix this, we can take the current ClusterIP service and configure it to be a LoadBalancer via YAML code
    - After running the apply, we are able to access localhost:9000 succesfully
- Ingress exposes HTTP and HTTPS routes outside the cluster to services within the cluster (inbound traffic)
- Ingress Features:
    - SSL/TLS Termination
    - External URLs (Fully Qualified Domain Names - FQDNs)
    - Path based routing
- When comparing the ingress vs the loadbalancer, both act as a way to direct traffic to a single source
    - The difference is that the ingress can be used for multiple services (frontend, backend)
    - The load balancer is intended to be only used for a single type of service (i.e frontend)
- The ingress resource is implemented via the Ingress Controler (NGINX, Traefik, Cilium, AGIC - Application Ingress Controller)
- Rancher Desktop uses Traefik

## Storage 
- When working with storage, we first took a look at recreating a pod
- When working with storage, note that volumes are stored in the POD not within the container
    - Instead of directly having them inside the container, they are referenced by mounting them in the container
- Using the `kubectl describe pod` command, we were able to see that the mealie pods contain volumes
- To try to mimic this, the kubernetes documentation was references to include a volumes object inside `spec`
- Additionally, containers who wanted to use the volume needed a `volumeMounts`
    - Within volumeMounts, you need to specify the mountPath (i.e /scratch) and name of the volume you're referencing
- Observe that we used emptyDir for this volume
    - This indicates that our volume is temporary or `Ephemeral`
    - Once the pod is deleted, the data stored inside the volume is cleared
- There is a difference between persistent volumes and persistent volumes claims
    - Persistent volumes is like a huge disk living inside your cluster
    - Several teams run applications that require chunks of that disk space
    - Persistent volume claims are responsible for "claiming" territory on the disk for the application
- Persistent volumes can be provisioned [beforehand](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) or dynamically provisioned using Storage Classes
- When working on this section, we chose to create the persistent volume claim before hand (see storage.yaml in mealie/)
- When running the `kubectl get persistentvolumeclaims` command, observe that the created volume claim is pending
    - This is because the persistent volume has not been created yet
- Going back into the mealie deployments.yaml file, we needed to use the `persistentVolumeClaim` object that specified the claimName to be mealie-data
- From there, the volume was mounted with the mountPath **/app/data**
- Observe that once the apply has been run, after running the `kubectl get persistentvolumeclaims` command that the status is now bound
- There are a lot of different storage classes - `kubectl get storageclasses.storage.k8s.io`
    - The way it works is that in order to get the storage for the persistent volumes + volume claims, there is a configuration that is able to take storage from my local machine to put into the kubernetes cluster
    - This was done through Rancher Desktop, but in the real world it would be either on prem or in the cloud
    - On prem would most likely be handled through a hypervisor layer (VMware, vSphere) which each contain their own storage class
    - In comparison, the cloud each has their own distinguishable storage classes (i.e AWS EBS/EFS)
- ReadAccessMode is a parameter that can be added to your volume to specify the access conditions of a PersistentVolume
    - ReadWriteOnce - RW access for one node 
    - ReadOnlyOnce - R Access for one node
    - ReadWriteMany - RW for many nodes
    - ReadWriteOncePod - RW for only one pod

## K9s
- Another command to use is `k9s` - This is what can be used to explore clusters
- You can navigate up and down against the pods using VIM navigation keys 
- There is additional information on each pods if you press the `->` Arrow key
- Pressing **0** shows all pods in the cluster
- `Shift + A` - Sorts pods by age
    - Can be useful to view recent deployments creating pods
- `Shift + S` - Sorts by status
    - Filter out all the pending/failed pods
- Going to a specific pod and clicking `l` - Logs
    - Turn on/off autoscaling `s`
- You can use vim to search for information from pods
    - Inside the homepage - typing `/mealie` searches for mealie pods
    - `/` + Enter resets the search
- Pressing `s` on a pod lets you shell into them
- Kill a pod with `Ctrl + K`
- Describe with `d`
- View yaml with `y`
- Enable port forwarding with `Shift + F`
    - It will display a F to indicate that forwarding is active
- Edit the service with `e`
- To switch between resources, type `:[resource]`
    - For example - `:pods`, `:svc`, `:deployments`
- **NOTE: k9s should not be used as a crutch for everything - On the CKA Exam you may not be able to use**

## Helm
- Helm is a package manager that can be used to help manage k8s deployments
- Helm does not run on your cluster
    - It is a binary that runs on your computer
    - You add repositories onto the binary to be used with the cluster
    - When ready to use the repositories, the binary connects to the cluster and performs actions for you
- GitOps tends to use Helm for the deployment of k8s resources
- In this example, we are going to use helm to deploy [homarr](https://homarr.dev/docs/getting-started/installation/helm/)
- `helm repo add [repo_name] [url]`
    - Adds the repository to the list of repositories on your local system
- `helm repo update`
    - Update the list of repositories with the repository you just added 
    - Similar to `git pull`
- `helm install [name] [repo_name]/[resource]`
    - Uses current namespace
    - You can override values setting flags `--namespace [namespace] --create-namespace`
    - `helm install homarr oben01/homarr --namespace homarr --create-namespace --values=values.yaml`
- `helm ls -A` - Lists all helm charts
- `helm uninstall -n [namespace] [chart]`
- `helm upgrade -n [namespace] [chart] [repository] --values=values.yaml`
